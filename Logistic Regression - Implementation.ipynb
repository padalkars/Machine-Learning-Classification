{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv(os.getcwd() + '/amazon_baby_subset.csv/amazon_baby_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reviews file\n",
    "important_words = pd.read_json(os.getcwd() + '/important_words.json/important_words.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>My daughter had her 1st baby over a year ago. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lamaze Peekaboo, I Love You</td>\n",
       "      <td>One of baby's first and favorite books, and it...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SoftPlay Peek-A-Boo Where's Elmo A Children's ...</td>\n",
       "      <td>Very cute interactive book! My son loves this ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  Stop Pacifier Sucking without tears with Thumb...   \n",
       "1    Nature's Lullabies Second Year Sticker Calendar   \n",
       "2    Nature's Lullabies Second Year Sticker Calendar   \n",
       "3                        Lamaze Peekaboo, I Love You   \n",
       "4  SoftPlay Peek-A-Boo Where's Elmo A Children's ...   \n",
       "\n",
       "                                              review  rating  sentiment  \n",
       "0  All of my kids have cried non-stop when I trie...       5          1  \n",
       "1  We wanted to get something to keep track of ou...       5          1  \n",
       "2  My daughter had her 1st baby over a year ago. ...       5          1  \n",
       "3  One of baby's first and favorite books, and it...       4          1  \n",
       "4  Very cute interactive book! My son loves this ...       5          1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53072, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0   baby\n",
       "1    one\n",
       "2  great\n",
       "3   love\n",
       "4    use"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuations\n",
    "table = str.maketrans(string.punctuation, len(string.punctuation)*\" \")\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products.fillna({'review':''})  # fill in N/A's in the review column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the remove_punctuation function on every element of the review column and assign the result to the new column review_clean. Note. Many data frame packages support apply operation for this type of task. Consult appropriate manuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['review_clean'] = list(map(remove_punctuation, products['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>All of my kids have cried non stop when I trie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>My daughter had her 1st baby over a year ago. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>My daughter had her 1st baby over a year ago  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lamaze Peekaboo, I Love You</td>\n",
       "      <td>One of baby's first and favorite books, and it...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>One of baby s first and favorite books  and it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SoftPlay Peek-A-Boo Where's Elmo A Children's ...</td>\n",
       "      <td>Very cute interactive book! My son loves this ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Very cute interactive book  My son loves this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  Stop Pacifier Sucking without tears with Thumb...   \n",
       "1    Nature's Lullabies Second Year Sticker Calendar   \n",
       "2    Nature's Lullabies Second Year Sticker Calendar   \n",
       "3                        Lamaze Peekaboo, I Love You   \n",
       "4  SoftPlay Peek-A-Boo Where's Elmo A Children's ...   \n",
       "\n",
       "                                              review  rating  sentiment  \\\n",
       "0  All of my kids have cried non-stop when I trie...       5          1   \n",
       "1  We wanted to get something to keep track of ou...       5          1   \n",
       "2  My daughter had her 1st baby over a year ago. ...       5          1   \n",
       "3  One of baby's first and favorite books, and it...       4          1   \n",
       "4  Very cute interactive book! My son loves this ...       5          1   \n",
       "\n",
       "                                        review_clean  \n",
       "0  All of my kids have cried non stop when I trie...  \n",
       "1  We wanted to get something to keep track of ou...  \n",
       "2  My daughter had her 1st baby over a year ago  ...  \n",
       "3  One of baby s first and favorite books  and it...  \n",
       "4  Very cute interactive book  My son loves this ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(important_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each word in important_words, we compute a count for the number of times the word occurs in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            baby\n",
       "1             one\n",
       "2           great\n",
       "3            love\n",
       "4             use\n",
       "5           would\n",
       "6            like\n",
       "7            easy\n",
       "8          little\n",
       "9            seat\n",
       "10            old\n",
       "11           well\n",
       "12            get\n",
       "13           also\n",
       "14         really\n",
       "15            son\n",
       "16           time\n",
       "17         bought\n",
       "18        product\n",
       "19           good\n",
       "20       daughter\n",
       "21           much\n",
       "22          loves\n",
       "23       stroller\n",
       "24            put\n",
       "25         months\n",
       "26            car\n",
       "27          still\n",
       "28           back\n",
       "29           used\n",
       "          ...    \n",
       "163       started\n",
       "164      anything\n",
       "165          last\n",
       "166       company\n",
       "167          come\n",
       "168      returned\n",
       "169         maybe\n",
       "170          took\n",
       "171         broke\n",
       "172         makes\n",
       "173          stay\n",
       "174       instead\n",
       "175          idea\n",
       "176          head\n",
       "177          said\n",
       "178          less\n",
       "179          went\n",
       "180       working\n",
       "181          high\n",
       "182          unit\n",
       "183         seems\n",
       "184       picture\n",
       "185    completely\n",
       "186          wish\n",
       "187        buying\n",
       "188        babies\n",
       "189           won\n",
       "190           tub\n",
       "191        almost\n",
       "192        either\n",
       "Name: 0, Length: 193, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function time in module time:\n",
      "\n",
      "time(...)\n",
      "    time() -> floating point number\n",
      "    \n",
      "    Return the current time in seconds since the Epoch.\n",
      "    Fractions of a second may be present if the system clock provides them.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(timeit.time.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required for lambda and apply:- 58.464 seconds\n"
     ]
    }
   ],
   "source": [
    "#Count the frequency of a word in the review.\n",
    "start_time = timeit.time.time()\n",
    "for word in important_words[0]:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))\n",
    "end_time = timeit.time.time()\n",
    "print(\"Time required for lambda and apply:- %.3f seconds\" %((end_time-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required for List Comprehension:- 55.441 seconds\n"
     ]
    }
   ],
   "source": [
    "def check_presence(word, text):\n",
    "    return (text.split().count(word))\n",
    "\n",
    "start_time = timeit.time.time()\n",
    "for word in important_words[0]:\n",
    "    #List Comprehension\n",
    "    products[word] = [check_presence(word, rev) for rev in products['review']] \n",
    "end_time = timeit.time.time()\n",
    "\n",
    "print(\"Time required for List Comprehension:- %.3f seconds\" %((end_time-start_time)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 in products.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>baby</th>\n",
       "      <th>one</th>\n",
       "      <th>great</th>\n",
       "      <th>love</th>\n",
       "      <th>use</th>\n",
       "      <th>...</th>\n",
       "      <th>seems</th>\n",
       "      <th>picture</th>\n",
       "      <th>completely</th>\n",
       "      <th>wish</th>\n",
       "      <th>buying</th>\n",
       "      <th>babies</th>\n",
       "      <th>won</th>\n",
       "      <th>tub</th>\n",
       "      <th>almost</th>\n",
       "      <th>either</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>All of my kids have cried non stop when I trie...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>My daughter had her 1st baby over a year ago. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>My daughter had her 1st baby over a year ago  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lamaze Peekaboo, I Love You</td>\n",
       "      <td>One of baby's first and favorite books, and it...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>One of baby s first and favorite books  and it...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SoftPlay Peek-A-Boo Where's Elmo A Children's ...</td>\n",
       "      <td>Very cute interactive book! My son loves this ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Very cute interactive book  My son loves this ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  Stop Pacifier Sucking without tears with Thumb...   \n",
       "1    Nature's Lullabies Second Year Sticker Calendar   \n",
       "2    Nature's Lullabies Second Year Sticker Calendar   \n",
       "3                        Lamaze Peekaboo, I Love You   \n",
       "4  SoftPlay Peek-A-Boo Where's Elmo A Children's ...   \n",
       "\n",
       "                                              review  rating  sentiment  \\\n",
       "0  All of my kids have cried non-stop when I trie...       5          1   \n",
       "1  We wanted to get something to keep track of ou...       5          1   \n",
       "2  My daughter had her 1st baby over a year ago. ...       5          1   \n",
       "3  One of baby's first and favorite books, and it...       4          1   \n",
       "4  Very cute interactive book! My son loves this ...       5          1   \n",
       "\n",
       "                                        review_clean  baby  one  great  love  \\\n",
       "0  All of my kids have cried non stop when I trie...     0    0      1     0   \n",
       "1  We wanted to get something to keep track of ou...     0    0      0     0   \n",
       "2  My daughter had her 1st baby over a year ago  ...     1    0      0     0   \n",
       "3  One of baby s first and favorite books  and it...     0    0      0     0   \n",
       "4  Very cute interactive book  My son loves this ...     0    0      1     0   \n",
       "\n",
       "   use   ...    seems  picture  completely  wish  buying  babies  won  tub  \\\n",
       "0    0   ...        0        0           0     0       0       0    0    0   \n",
       "1    0   ...        0        0           0     0       0       0    0    0   \n",
       "2    0   ...        0        0           0     0       0       0    0    0   \n",
       "3    0   ...        0        0           0     0       0       0    0    0   \n",
       "4    0   ...        0        0           0     0       0       1    0    0   \n",
       "\n",
       "   almost  either  \n",
       "0       0       0  \n",
       "1       0       0  \n",
       "2       0       0  \n",
       "3       0       0  \n",
       "4       0       0  \n",
       "\n",
       "[5 rows x 198 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz Question. How many reviews contain the word perfect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165\n"
     ]
    }
   ],
   "source": [
    "if('perfect' in products.columns):\n",
    "    print(products.loc[products['perfect']==1,'perfect'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2165, 198)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.loc[products['perfect']==1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data frame to multi-dimensional array\n",
    "def get_numpy_data(dataframe, features, label):\n",
    "    dataframe['constant'] = 1\n",
    "    feature_matrix = dataframe[['constant'] + list(features)]\n",
    "\n",
    "    feature_matrix, label_array =  list(map(lambda x:x.as_matrix(), \n",
    "                                       [feature_matrix, dataframe[label]])) \n",
    "    return (feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shrikrishna/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "feature_matrix, sentiment = get_numpy_data(products, important_words[0], 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>constant</th>\n",
       "      <th>baby</th>\n",
       "      <th>one</th>\n",
       "      <th>great</th>\n",
       "      <th>love</th>\n",
       "      <th>use</th>\n",
       "      <th>would</th>\n",
       "      <th>like</th>\n",
       "      <th>easy</th>\n",
       "      <th>little</th>\n",
       "      <th>...</th>\n",
       "      <th>seems</th>\n",
       "      <th>picture</th>\n",
       "      <th>completely</th>\n",
       "      <th>wish</th>\n",
       "      <th>buying</th>\n",
       "      <th>babies</th>\n",
       "      <th>won</th>\n",
       "      <th>tub</th>\n",
       "      <th>almost</th>\n",
       "      <th>either</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   constant  baby  one  great  love  use  would  like  easy  little   ...    \\\n",
       "0         1     0    0      1     0    0      0     0     1       0   ...     \n",
       "1         1     0    0      0     0    0      0     0     0       0   ...     \n",
       "2         1     1    0      0     0    0      0     0     0       0   ...     \n",
       "3         1     0    0      0     0    0      0     1     0       0   ...     \n",
       "4         1     0    0      1     0    0      0     0     0       0   ...     \n",
       "\n",
       "   seems  picture  completely  wish  buying  babies  won  tub  almost  either  \n",
       "0      0        0           0     0       0       0    0    0       0       0  \n",
       "1      0        0           0     0       0       0    0    0       0       0  \n",
       "2      0        0           0     0       0       0    0    0       0       0  \n",
       "3      0        0           0     0       0       0    0    0       0       0  \n",
       "4      0        0           0     0       0       1    0    0       0       0  \n",
       "\n",
       "[5 rows x 194 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[['constant'] + important_words[0].tolist()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz Question: How many features are there in the feature_matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53072, 194)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "194"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz Question: Assuming that the intercept is present, how does the number of features in feature_matrix relate to the number of features in the logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Both are same\n",
    "#We learn the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement the Link Function\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "        \n",
    "    score = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    probability_positive = ((1+np.e**(-1*score))**-1)\n",
    "    \n",
    "    return (probability_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] [1 2 3]\n",
      "3.7416573867739413 3.7416573867739413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def cosine_similarity(a, b):\n",
    "    #a and b are arrays\n",
    "    def find_mod(vec):\n",
    "        return (sum([ele**2 for ele in vec]))**(0.5)\n",
    "    \n",
    "    mod_a, mod_b = list(map(find_mod, [a,b]))\n",
    "    \n",
    "    print(mod_a, mod_b)\n",
    "    return (np.dot(a,b)/(mod_a*mod_b))\n",
    "\n",
    "A = np.array([1,2,3])\n",
    "B = np.array([1, 2, 3])\n",
    "#A, B = list(map(lambda x: np.array, [A,B]))\n",
    "print(A, B)\n",
    "cosine_similarity(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute derivative of log likelihood with respect to a single coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):\n",
    "    return (np.dot(feature, errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator = pd.Series(sentiment).map({-1:0, 1:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Likelihood\n",
    "\n",
    "In the main lecture, our focus was on the likelihood. In the advanced optional video, however, we introduced a transformation of this likelihood---called the log-likelihood---that simplifies the derivation of the gradient and is more numerically stable. Due to its numerical stability, we will use the log-likelihood instead of the likelihood to assess the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(feature_matrix, sentiment, coefficients):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores)))\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking gradient steps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feature_matrix: 2D array of features\n",
    "sentiment: 1D array of class labels\n",
    "initial_coefficients: 1D array containing initial values of coefficients\n",
    "step_size: a parameter controlling the size of the gradient steps\n",
    "max_iter: number of iterations to run gradient ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(feature_matrix, sentiment, initial_coefficients,\n",
    "                        step_size, max_iter):\n",
    "    coefficients = np.array(initial_coefficients)\n",
    "    \n",
    "    for itr in range(max_iter):\n",
    "        #coeff = initial_coefficients[i]\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "        prediction = predict_probability(feature_matrix,\n",
    "                                         coefficients)\n",
    "        \n",
    "        errors = np.array(indicator) - prediction\n",
    "        \n",
    "        for j in range(len(coefficients)):\n",
    "            derivative = feature_derivative(errors,\n",
    "                                            feature_matrix[:, j])\n",
    "            coefficients[j] += step_size*derivative\n",
    "                    \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood(feature_matrix, sentiment, coefficients)\n",
    "            print ('iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "        \n",
    "    return (coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Now, let us run the logistic regression solver with the parameters below:\n",
    "\n",
    "    feature_matrix = feature_matrix extracted\n",
    "    sentiment = sentiment extracted\n",
    "    initial_coefficients = a 194-dimensional vector filled with zeros\n",
    "    step_size = 1e-7\n",
    "    max_iter = 301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_coefficients = np.zeros(194)\n",
    "step_size = 1e-7\n",
    "max_iter = 301\n",
    "initial_coefficients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following outputs must match \n",
      "------------------------------------------------\n",
      "correct_predictions           = [0.98201379 0.26894142]\n",
      "output of predict_probability = [0.98201379 0.26894142]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dummy_feature_matrix = np.array([[1.,2.,3.], [1.,-1.,-1]])\n",
    "dummy_coefficients = np.array([1., 3., -1.])\n",
    "\n",
    "correct_scores      = np.array( [ 1.*1. + 2.*3. + 3.*(-1.),          1.*1. + (-1.)*3. + (-1.)*(-1.) ] )\n",
    "correct_predictions = np.array( [ 1./(1+np.exp(-correct_scores[0])), 1./(1+np.exp(-correct_scores[1])) ] )\n",
    "\n",
    "print ('The following outputs must match ')\n",
    "print ('------------------------------------------------')\n",
    "print ('correct_predictions           =', correct_predictions)\n",
    "print ('output of predict_probability =', predict_probability(dummy_feature_matrix, dummy_coefficients))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_sentiment = np.array([1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3314116154360326"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_log_likelihood(dummy_feature_matrix, dummy_sentiment, dummy_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53072, 194)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following outputs must match \n",
      "------------------------------------------------\n",
      "correct_log_likelihood           = -5.331411615436032\n",
      "output of compute_log_likelihood = -5.331411615436032\n"
     ]
    }
   ],
   "source": [
    "dummy_feature_matrix = np.array([[1.,2.,3.], [1.,-1.,-1]])\n",
    "dummy_coefficients = np.array([1., 3., -1.])\n",
    "dummy_sentiment = np.array([-1, 1])\n",
    "\n",
    "correct_indicators  = np.array( [ -1==+1,                                       1==+1 ] )\n",
    "correct_scores      = np.array( [ 1.*1. + 2.*3. + 3.*(-1.),                     1.*1. + (-1.)*3. + (-1.)*(-1.) ] )\n",
    "correct_first_term  = np.array( [ (correct_indicators[0]-1)*correct_scores[0],  (correct_indicators[1]-1)*correct_scores[1] ] )\n",
    "correct_second_term = np.array( [ np.log(1. + np.exp(-correct_scores[0])),      np.log(1. + np.exp(-correct_scores[1])) ] )\n",
    "\n",
    "correct_ll          =      sum( [ correct_first_term[0]-correct_second_term[0], correct_first_term[1]-correct_second_term[1] ] ) \n",
    "\n",
    "print ('The following outputs must match ')\n",
    "print ('------------------------------------------------')\n",
    "print ('correct_log_likelihood           =', correct_ll)\n",
    "print ('output of compute_log_likelihood =', compute_log_likelihood(dummy_feature_matrix, dummy_sentiment, dummy_coefficients))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53072, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(indicator).reshape(53072, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_prediction = predict_probability(dummy_feature_matrix, dummy_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_indicator = np.array([-1==+1, +1==+1])\n",
    "dummy_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98201379, 0.26894142])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_error = dummy_indicator - dummy_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_result = pd.DataFrame({'Indicator':dummy_indicator, \n",
    "                             'Prediction':dummy_prediction,\n",
    "                             'Error':dummy_error},\n",
    "                             columns = ['Indicator', 'Prediction', 'Error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Indicator</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.982014</td>\n",
       "      <td>-0.982014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.268941</td>\n",
       "      <td>0.731059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Indicator  Prediction     Error\n",
       "0      False    0.982014 -0.982014\n",
       "1       True    0.268941  0.731059"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_coefficients = np.zeros(194)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -36782.24149905\n",
      "iteration   1: log likelihood of observed labels = -36777.77993493\n",
      "iteration   2: log likelihood of observed labels = -36773.32246359\n",
      "iteration   3: log likelihood of observed labels = -36768.86907436\n",
      "iteration   4: log likelihood of observed labels = -36764.41975666\n",
      "iteration   5: log likelihood of observed labels = -36759.97449997\n",
      "iteration   6: log likelihood of observed labels = -36755.53329383\n",
      "iteration   7: log likelihood of observed labels = -36751.09612785\n",
      "iteration   8: log likelihood of observed labels = -36746.66299174\n",
      "iteration   9: log likelihood of observed labels = -36742.23387522\n",
      "iteration  10: log likelihood of observed labels = -36737.80876812\n",
      "iteration  11: log likelihood of observed labels = -36733.38766031\n",
      "iteration  12: log likelihood of observed labels = -36728.97054176\n",
      "iteration  13: log likelihood of observed labels = -36724.55740245\n",
      "iteration  14: log likelihood of observed labels = -36720.14823248\n",
      "iteration  15: log likelihood of observed labels = -36715.74302197\n",
      "iteration  20: log likelihood of observed labels = -36693.77602065\n",
      "iteration  30: log likelihood of observed labels = -36650.13348177\n",
      "iteration  40: log likelihood of observed labels = -36606.87197329\n",
      "iteration  50: log likelihood of observed labels = -36563.98286427\n",
      "iteration  60: log likelihood of observed labels = -36521.45802595\n",
      "iteration  70: log likelihood of observed labels = -36479.28979071\n",
      "iteration  80: log likelihood of observed labels = -36437.47091460\n",
      "iteration  90: log likelihood of observed labels = -36395.99454329\n",
      "iteration 100: log likelihood of observed labels = -36354.85418097\n",
      "iteration 200: log likelihood of observed labels = -35960.70841629\n",
      "iteration 300: log likelihood of observed labels = -35594.84598974\n"
     ]
    }
   ],
   "source": [
    "\n",
    "coefficients = logistic_regression(feature_matrix, sentiment, initial_coefficients,\n",
    "                        step_size, max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.73673054e-03,  1.36807710e-02, -5.24032888e-03,  5.06832503e-02,\n",
       "        6.51628882e-02,  7.55938637e-03, -5.38797585e-02, -4.01267488e-03,\n",
       "        6.20295470e-02,  4.49642128e-02,  1.73838517e-03,  1.47559181e-02,\n",
       "        2.06982177e-02, -2.93239973e-02,  1.37731585e-02,  1.01583220e-03,\n",
       "        1.06265309e-02, -1.70365851e-02, -1.22182219e-02, -3.61762595e-02,\n",
       "        2.14703057e-03,  1.64555566e-02, -5.82086591e-03,  4.48773640e-02,\n",
       "        7.20303166e-03,  9.41287916e-04, -1.64434290e-05,  9.53392509e-03,\n",
       "        8.11709255e-03, -1.94594532e-02,  1.30086724e-03,  1.18890164e-02,\n",
       "       -1.14665743e-02, -3.02141443e-02,  2.26859843e-02,  1.65406585e-02,\n",
       "        7.08093759e-04, -1.05719575e-02, -7.41837552e-04, -1.02562826e-02,\n",
       "       -2.81967169e-03,  5.41935965e-03,  4.59599698e-03,  4.90897395e-03,\n",
       "       -2.48482415e-04,  3.62836558e-03, -2.76060363e-03, -1.94944998e-02,\n",
       "        1.24059131e-02,  1.31786668e-03, -1.89826154e-02,  6.95054917e-03,\n",
       "        1.26710591e-02, -6.77790580e-03, -5.97042721e-03,  6.47969177e-03,\n",
       "        7.72992267e-03, -3.47238612e-04,  5.73730164e-03,  6.51899207e-03,\n",
       "        4.12399718e-03, -5.47650020e-03,  6.33719598e-03,  8.20728952e-03,\n",
       "        1.57341386e-03,  3.93240886e-03,  6.27488925e-03, -1.74694253e-02,\n",
       "       -5.85705998e-03, -1.06369479e-02, -1.01356787e-02,  5.54683496e-03,\n",
       "        3.36095552e-03, -5.98090977e-03,  3.00697222e-03,  2.60769334e-03,\n",
       "        1.39306757e-02, -1.26308637e-03, -2.09466657e-02, -4.43246976e-03,\n",
       "       -4.12376748e-03,  6.58302277e-03, -3.13557799e-04,  1.55589512e-02,\n",
       "        9.81028326e-03, -8.85325072e-05,  2.88419883e-03,  6.54353892e-03,\n",
       "        1.10810298e-02,  1.34560471e-03,  3.75994431e-03,  1.70820809e-02,\n",
       "       -4.35288195e-03,  1.19635508e-03, -1.11749286e-02,  7.80033482e-03,\n",
       "       -6.85844390e-04, -2.23526167e-02, -2.06818112e-02, -1.91635656e-02,\n",
       "       -2.07564252e-02, -1.30054050e-02, -1.36551429e-02, -1.29099479e-02,\n",
       "       -1.28907736e-02, -3.17052757e-03, -1.97327582e-02, -1.09072560e-02,\n",
       "       -1.22625586e-02, -1.09846752e-02, -5.33415398e-03, -3.98580437e-03,\n",
       "       -6.31154723e-03, -2.31553451e-02, -2.45706626e-02, -1.48594253e-03,\n",
       "       -4.13158491e-03, -6.26455536e-03, -3.96444127e-03, -3.37876921e-03,\n",
       "       -3.74984645e-03, -5.77811361e-03, -1.06775290e-02, -1.19657115e-02,\n",
       "       -1.91380643e-03, -5.74816233e-03, -1.17513309e-03, -2.52680530e-03,\n",
       "       -1.72259720e-03, -4.16074774e-03, -7.09465398e-03, -5.97377838e-03,\n",
       "       -4.75767719e-03, -5.70303778e-03, -1.03410910e-02, -6.86243213e-03,\n",
       "       -2.21660451e-03, -6.76947051e-03, -4.95408287e-03, -7.39463813e-03,\n",
       "       -9.51947266e-03, -6.23697919e-03, -7.31722245e-04, -1.52826461e-04,\n",
       "        4.34254255e-04,  2.52319939e-03, -5.72122795e-03,  1.88195285e-03,\n",
       "       -4.35013592e-03, -2.81148620e-03, -4.33030578e-03, -1.91463067e-03,\n",
       "       -5.88850240e-04, -2.50433406e-03,  3.38143331e-04, -2.87250556e-03,\n",
       "       -9.65147948e-03, -7.18730122e-03,  1.69270966e-03, -9.83420556e-03,\n",
       "       -9.86430878e-03, -2.35345141e-03, -4.13314289e-05, -3.63464014e-03,\n",
       "       -6.24578967e-03, -5.33148760e-03, -4.82284842e-03, -1.01359014e-02,\n",
       "       -3.23229331e-03, -1.47633054e-02, -6.06320700e-03, -4.29896266e-03,\n",
       "       -1.20786354e-02,  2.27463694e-03, -8.69650475e-03, -1.92816490e-03,\n",
       "       -8.75923536e-03, -2.59010422e-03, -6.25630433e-03, -5.37941890e-03,\n",
       "       -6.87079055e-03, -7.04194590e-03, -2.49470325e-05, -9.34371269e-03,\n",
       "        2.94335957e-03, -6.63661091e-03, -9.19449619e-03,  3.83170039e-03,\n",
       "       -5.01508767e-03,  1.48132201e-03,  1.21492548e-04, -5.18521192e-03,\n",
       "       -1.05311256e-03, -3.69495127e-03])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz question: As each iteration of gradient ascent passes, does the log likelihood increase or decrease? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood Increases\n"
     ]
    }
   ],
   "source": [
    "print(\"Log Likelihood Increases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting sentiments\n",
    "\n",
    "15. Recall from lecture that class predictions for a data point x can be computed from the coefficients w using the following formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(feature_mat, coeff):\n",
    "    score = np.dot(feature_mat, coeff)\n",
    "    \n",
    "    #According to Logistic Function\n",
    "    if(score>0):\n",
    "        return (+1)\n",
    "    else:\n",
    "        return (-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz question: How many reviews were predicted to have positive sentiment? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predicted positive sentiments:- 24714\n"
     ]
    }
   ],
   "source": [
    "predicted_sentiments = [predict_sentiment(feature_matrix[i,:], coefficients)\n",
    "                        for i in range(len(feature_matrix))]\n",
    "print(\"Number of predicted positive sentiments:- \"+\n",
    "      str(len([ele for ele in predicted_sentiments if(ele==1)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53072, 194)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predicted negative sentiments:- 28358\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of predicted negative sentiments:- \"+\n",
    "      str(len([ele for ele in predicted_sentiments if(ele==-1)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz question: What is the accuracy of the model on predictions made above? (round to 2 digits of accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'sentiment' in products.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual  Predicted\n",
       "0       1          1\n",
       "1       1         -1\n",
       "2       1          1\n",
       "3       1          1\n",
       "4       1          1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy  = #Sentiments Predicted as Positive / Total # Observations\n",
    "#Actually positive sentiments\n",
    "actual_sentiments = products['sentiment']\n",
    "\n",
    "results = pd.DataFrame({'Actual': actual_sentiments, 'Predicted': predicted_sentiments},\n",
    "                       columns=['Actual', 'Predicted'])\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(a, b):\n",
    "    if(a==b):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_predictions = [calculate_accuracy(act, pred) \n",
    "                             for act, pred in zip(results['Actual'],\n",
    "                                                  results['Predicted'])]\n",
    "results_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:- 0.74\n"
     ]
    }
   ],
   "source": [
    "#Accuracy \n",
    "correct_count = len([ele for ele in results_predictions if(ele==1)])\n",
    "print(\"Accuracy:- %.2f\" %(correct_count/len(results_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which words contribute most to positive & negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['baby', 'one', 'great', 'love', 'use', 'would', 'like', 'easy',\n",
       "       'little', 'seat', 'old', 'well', 'get', 'also', 'really', 'son',\n",
       "       'time', 'bought', 'product', 'good', 'daughter', 'much', 'loves',\n",
       "       'stroller', 'put', 'months', 'car', 'still', 'back', 'used',\n",
       "       'recommend', 'first', 'even', 'perfect', 'nice', 'bag', 'two',\n",
       "       'using', 'got', 'fit', 'around', 'diaper', 'enough', 'month',\n",
       "       'price', 'go', 'could', 'soft', 'since', 'buy', 'room', 'works',\n",
       "       'made', 'child', 'keep', 'size', 'small', 'need', 'year', 'big',\n",
       "       'make', 'take', 'easily', 'think', 'crib', 'clean', 'way',\n",
       "       'quality', 'thing', 'better', 'without', 'set', 'new', 'every',\n",
       "       'cute', 'best', 'bottles', 'work', 'purchased', 'right', 'lot',\n",
       "       'side', 'happy', 'comfortable', 'toy', 'able', 'kids', 'bit',\n",
       "       'night', 'long', 'fits', 'see', 'us', 'another', 'play', 'day',\n",
       "       'money', 'monitor', 'tried', 'thought', 'never', 'item', 'hard',\n",
       "       'plastic', 'however', 'disappointed', 'reviews', 'something',\n",
       "       'going', 'pump', 'bottle', 'cup', 'waste', 'return', 'amazon',\n",
       "       'different', 'top', 'want', 'problem', 'know', 'water', 'try',\n",
       "       'received', 'sure', 'times', 'chair', 'find', 'hold', 'gate',\n",
       "       'open', 'bottom', 'away', 'actually', 'cheap', 'worked', 'getting',\n",
       "       'ordered', 'came', 'milk', 'bad', 'part', 'worth', 'found',\n",
       "       'cover', 'many', 'design', 'looking', 'weeks', 'say', 'wanted',\n",
       "       'look', 'place', 'purchase', 'looks', 'second', 'piece', 'box',\n",
       "       'pretty', 'trying', 'difficult', 'together', 'though', 'give',\n",
       "       'started', 'anything', 'last', 'company', 'come', 'returned',\n",
       "       'maybe', 'took', 'broke', 'makes', 'stay', 'instead', 'idea',\n",
       "       'head', 'said', 'less', 'went', 'working', 'high', 'unit', 'seems',\n",
       "       'picture', 'completely', 'wish', 'buying', 'babies', 'won', 'tub',\n",
       "       'almost', 'either'], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_words[0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baby', 0.013680770964074688),\n",
       " ('one', -0.005240328876921257),\n",
       " ('great', 0.0506832503387355),\n",
       " ('love', 0.06516288823238833),\n",
       " ('use', 0.0075593863720944815),\n",
       " ('would', -0.0538797585447843),\n",
       " ('like', -0.004012674876131312),\n",
       " ('easy', 0.062029546990689015),\n",
       " ('little', 0.04496421284998458),\n",
       " ('seat', 0.0017383851737173646),\n",
       " ('old', 0.014755918079191233),\n",
       " ('well', 0.02069821773359),\n",
       " ('get', -0.02932399727767066),\n",
       " ('also', 0.01377315851278538),\n",
       " ('really', 0.0010158322026321705),\n",
       " ('son', 0.010626530928533847),\n",
       " ('time', -0.017036585087027243),\n",
       " ('bought', -0.01221822188862161),\n",
       " ('product', -0.03617625947473382),\n",
       " ('good', 0.002147030566264864),\n",
       " ('daughter', 0.01645555655729781),\n",
       " ('much', -0.005820865907973413),\n",
       " ('loves', 0.04487736399389728),\n",
       " ('stroller', 0.007203031662434025),\n",
       " ('put', 0.000941287915687742),\n",
       " ('months', -1.644342902241027e-05),\n",
       " ('car', 0.009533925094999023),\n",
       " ('still', 0.008117092546400051),\n",
       " ('back', -0.019459453150996592),\n",
       " ('used', 0.0013008672352591564),\n",
       " ('recommend', 0.011889016377716874),\n",
       " ('first', -0.011466574278205167),\n",
       " ('even', -0.030214144320453017),\n",
       " ('perfect', 0.022685984298359947),\n",
       " ('nice', 0.016540658492544564),\n",
       " ('bag', 0.0007080937586678225),\n",
       " ('two', -0.010571957519032102),\n",
       " ('using', -0.0007418375519610519),\n",
       " ('got', -0.010256282648006224),\n",
       " ('fit', -0.0028196716853235537),\n",
       " ('around', 0.00541935965230483),\n",
       " ('diaper', 0.004595996975375314),\n",
       " ('enough', 0.004908973954285623),\n",
       " ('month', -0.0002484824154075256),\n",
       " ('price', 0.003628365581008195),\n",
       " ('go', -0.002760603631110178),\n",
       " ('could', -0.01949449976413961),\n",
       " ('soft', 0.012405913106793417),\n",
       " ('since', 0.0013178666756072676),\n",
       " ('buy', -0.01898261536532469),\n",
       " ('room', 0.0069505491719928075),\n",
       " ('works', 0.012671059093092074),\n",
       " ('made', -0.006777905800962794),\n",
       " ('child', -0.0059704272093479595),\n",
       " ('keep', 0.006479691768581875),\n",
       " ('size', 0.007729922669456849),\n",
       " ('small', -0.00034723861216987435),\n",
       " ('need', 0.005737301642100271),\n",
       " ('year', 0.006518992070944301),\n",
       " ('big', 0.004123997183189743),\n",
       " ('make', -0.00547650019560423),\n",
       " ('take', 0.006337195975409497),\n",
       " ('easily', 0.008207289516803398),\n",
       " ('think', 0.0015734138551054303),\n",
       " ('crib', 0.00393240886412322),\n",
       " ('clean', 0.006274889251764406),\n",
       " ('way', -0.017469425344553206),\n",
       " ('quality', -0.005857059980750963),\n",
       " ('thing', -0.010636947932972407),\n",
       " ('better', -0.0101356787362247),\n",
       " ('without', 0.005546834960080509),\n",
       " ('set', 0.0033609555206205962),\n",
       " ('new', -0.005980909765771513),\n",
       " ('every', 0.0030069722224196076),\n",
       " ('cute', 0.002607693343217017),\n",
       " ('best', 0.013930675730989597),\n",
       " ('bottles', -0.0012630863681651213),\n",
       " ('work', -0.02094666567861414),\n",
       " ('purchased', -0.004432469763390717),\n",
       " ('right', -0.004123767476740157),\n",
       " ('lot', 0.006583022773509948),\n",
       " ('side', -0.0003135577986450468),\n",
       " ('happy', 0.015558951246856233),\n",
       " ('comfortable', 0.009810283260449552),\n",
       " ('toy', -8.853250721200222e-05),\n",
       " ('able', 0.002884198833998323),\n",
       " ('kids', 0.006543538919441802),\n",
       " ('bit', 0.011081029826280007),\n",
       " ('night', 0.001345604713096243),\n",
       " ('long', 0.003759944305666101),\n",
       " ('fits', 0.01708208087034107),\n",
       " ('see', -0.004352881952205973),\n",
       " ('us', 0.001196355083246034),\n",
       " ('another', -0.011174928562632871),\n",
       " ('play', 0.007800334817552406),\n",
       " ('day', -0.0006858443895454593),\n",
       " ('money', -0.0223526167156249),\n",
       " ('monitor', -0.02068181121460328),\n",
       " ('tried', -0.019163565631510185),\n",
       " ('thought', -0.020756425233543412),\n",
       " ('never', -0.013005405035213156),\n",
       " ('item', -0.01365514290056185),\n",
       " ('hard', -0.012909947911458164),\n",
       " ('plastic', -0.012890773552133212),\n",
       " ('however', -0.0031705275712253936),\n",
       " ('disappointed', -0.019732758233784013),\n",
       " ('reviews', -0.010907255968065395),\n",
       " ('something', -0.012262558606932185),\n",
       " ('going', -0.01098467522304984),\n",
       " ('pump', -0.005334153984694427),\n",
       " ('bottle', -0.003985804372689815),\n",
       " ('cup', -0.006311547231173567),\n",
       " ('waste', -0.02315534512199584),\n",
       " ('return', -0.024570662566524983),\n",
       " ('amazon', -0.001485942527391314),\n",
       " ('different', -0.00413158491060751),\n",
       " ('top', -0.006264555364781907),\n",
       " ('want', -0.003964441269037393),\n",
       " ('problem', -0.003378769211681065),\n",
       " ('know', -0.0037498464466025785),\n",
       " ('water', -0.005778113611496449),\n",
       " ('try', -0.010677528961328389),\n",
       " ('received', -0.011965711520728846),\n",
       " ('sure', -0.0019138064277149677),\n",
       " ('times', -0.005748162325826816),\n",
       " ('chair', -0.0011751330912611487),\n",
       " ('find', -0.0025268053015301568),\n",
       " ('hold', -0.0017225972014805577),\n",
       " ('gate', -0.004160747744827084),\n",
       " ('open', -0.007094653976564978),\n",
       " ('bottom', -0.005973778383658588),\n",
       " ('away', -0.004757677185286699),\n",
       " ('actually', -0.005703037784686172),\n",
       " ('cheap', -0.010341090995988716),\n",
       " ('worked', -0.006862432126523159),\n",
       " ('getting', -0.0022166045075397807),\n",
       " ('ordered', -0.006769470510477964),\n",
       " ('came', -0.004954082869785295),\n",
       " ('milk', -0.007394638131350184),\n",
       " ('bad', -0.009519472664880905),\n",
       " ('part', -0.006236979186493673),\n",
       " ('worth', -0.0007317222452870946),\n",
       " ('found', -0.00015282646115140895),\n",
       " ('cover', 0.00043425425471770024),\n",
       " ('many', 0.002523199385541257),\n",
       " ('design', -0.005721227947572743),\n",
       " ('looking', 0.0018819528490675271),\n",
       " ('weeks', -0.004350135918631167),\n",
       " ('say', -0.0028114861958062876),\n",
       " ('wanted', -0.004330305782738801),\n",
       " ('look', -0.0019146306704767376),\n",
       " ('place', -0.0005888502404132606),\n",
       " ('purchase', -0.0025043340581670363),\n",
       " ('looks', 0.0003381433306215629),\n",
       " ('second', -0.0028725055596068487),\n",
       " ('piece', -0.009651479482034419),\n",
       " ('box', -0.007187301221822917),\n",
       " ('pretty', 0.0016927096601869372),\n",
       " ('trying', -0.009834205560115935),\n",
       " ('difficult', -0.009864308777979669),\n",
       " ('together', -0.002353451414287863),\n",
       " ('though', -4.1331428936637885e-05),\n",
       " ('give', -0.003634640138328766),\n",
       " ('started', -0.006245789674640817),\n",
       " ('anything', -0.005331487603879164),\n",
       " ('last', -0.004822848415602177),\n",
       " ('company', -0.010135901443394092),\n",
       " ('come', -0.003232293307448333),\n",
       " ('returned', -0.014763305420310346),\n",
       " ('maybe', -0.00606320699663826),\n",
       " ('took', -0.004298962662432533),\n",
       " ('broke', -0.01207863540345767),\n",
       " ('makes', 0.002274636944860159),\n",
       " ('stay', -0.008696504750903321),\n",
       " ('instead', -0.0019281649032273855),\n",
       " ('idea', -0.008759235359703656),\n",
       " ('head', -0.0025901042191508426),\n",
       " ('said', -0.006256304333645085),\n",
       " ('less', -0.00537941890232329),\n",
       " ('went', -0.006870790546275687),\n",
       " ('working', -0.007041945901153443),\n",
       " ('high', -2.4947032511550998e-05),\n",
       " ('unit', -0.009343712689748292),\n",
       " ('seems', 0.002943359568634231),\n",
       " ('picture', -0.006636610914255245),\n",
       " ('completely', -0.009194496191269351),\n",
       " ('wish', 0.0038317003921609786),\n",
       " ('buying', -0.005015087667465265),\n",
       " ('babies', 0.0014813220114542579),\n",
       " ('won', 0.00012149254826496427),\n",
       " ('tub', -0.005185211917236045),\n",
       " ('almost', -0.0010531125618072682),\n",
       " ('either', -0.0036949512652424852)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Positive Reviews\n",
    "'''\n",
    "The list containing important words is present.\n",
    "'''\n",
    "words_coeff_tuple = [(word, coefficient) for word, coefficient in zip(important_words[0].ravel(), coefficients[1:])]\n",
    "words_coeff_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_coeff_tuple = sorted(words_coeff_tuple, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 0.06516288823238833),\n",
       " ('easy', 0.062029546990689015),\n",
       " ('great', 0.0506832503387355),\n",
       " ('little', 0.04496421284998458),\n",
       " ('loves', 0.04487736399389728),\n",
       " ('perfect', 0.022685984298359947),\n",
       " ('well', 0.02069821773359),\n",
       " ('fits', 0.01708208087034107),\n",
       " ('nice', 0.016540658492544564),\n",
       " ('daughter', 0.01645555655729781),\n",
       " ('happy', 0.015558951246856233),\n",
       " ('old', 0.014755918079191233),\n",
       " ('best', 0.013930675730989597),\n",
       " ('also', 0.01377315851278538),\n",
       " ('baby', 0.013680770964074688),\n",
       " ('works', 0.012671059093092074),\n",
       " ('soft', 0.012405913106793417),\n",
       " ('recommend', 0.011889016377716874),\n",
       " ('bit', 0.011081029826280007),\n",
       " ('son', 0.010626530928533847),\n",
       " ('comfortable', 0.009810283260449552),\n",
       " ('car', 0.009533925094999023),\n",
       " ('easily', 0.008207289516803398),\n",
       " ('still', 0.008117092546400051),\n",
       " ('play', 0.007800334817552406),\n",
       " ('size', 0.007729922669456849),\n",
       " ('use', 0.0075593863720944815),\n",
       " ('stroller', 0.007203031662434025),\n",
       " ('room', 0.0069505491719928075),\n",
       " ('lot', 0.006583022773509948),\n",
       " ('kids', 0.006543538919441802),\n",
       " ('year', 0.006518992070944301),\n",
       " ('keep', 0.006479691768581875),\n",
       " ('take', 0.006337195975409497),\n",
       " ('clean', 0.006274889251764406),\n",
       " ('need', 0.005737301642100271),\n",
       " ('without', 0.005546834960080509),\n",
       " ('around', 0.00541935965230483),\n",
       " ('enough', 0.004908973954285623),\n",
       " ('diaper', 0.004595996975375314),\n",
       " ('big', 0.004123997183189743),\n",
       " ('crib', 0.00393240886412322),\n",
       " ('wish', 0.0038317003921609786),\n",
       " ('long', 0.003759944305666101),\n",
       " ('price', 0.003628365581008195),\n",
       " ('set', 0.0033609555206205962),\n",
       " ('every', 0.0030069722224196076),\n",
       " ('seems', 0.002943359568634231),\n",
       " ('able', 0.002884198833998323),\n",
       " ('cute', 0.002607693343217017),\n",
       " ('many', 0.002523199385541257),\n",
       " ('makes', 0.002274636944860159),\n",
       " ('good', 0.002147030566264864),\n",
       " ('looking', 0.0018819528490675271),\n",
       " ('seat', 0.0017383851737173646),\n",
       " ('pretty', 0.0016927096601869372),\n",
       " ('think', 0.0015734138551054303),\n",
       " ('babies', 0.0014813220114542579),\n",
       " ('night', 0.001345604713096243),\n",
       " ('since', 0.0013178666756072676),\n",
       " ('used', 0.0013008672352591564),\n",
       " ('us', 0.001196355083246034),\n",
       " ('really', 0.0010158322026321705),\n",
       " ('put', 0.000941287915687742),\n",
       " ('bag', 0.0007080937586678225),\n",
       " ('cover', 0.00043425425471770024),\n",
       " ('looks', 0.0003381433306215629),\n",
       " ('won', 0.00012149254826496427),\n",
       " ('months', -1.644342902241027e-05),\n",
       " ('high', -2.4947032511550998e-05),\n",
       " ('though', -4.1331428936637885e-05),\n",
       " ('toy', -8.853250721200222e-05),\n",
       " ('found', -0.00015282646115140895),\n",
       " ('month', -0.0002484824154075256),\n",
       " ('side', -0.0003135577986450468),\n",
       " ('small', -0.00034723861216987435),\n",
       " ('place', -0.0005888502404132606),\n",
       " ('day', -0.0006858443895454593),\n",
       " ('worth', -0.0007317222452870946),\n",
       " ('using', -0.0007418375519610519),\n",
       " ('almost', -0.0010531125618072682),\n",
       " ('chair', -0.0011751330912611487),\n",
       " ('bottles', -0.0012630863681651213),\n",
       " ('amazon', -0.001485942527391314),\n",
       " ('hold', -0.0017225972014805577),\n",
       " ('sure', -0.0019138064277149677),\n",
       " ('look', -0.0019146306704767376),\n",
       " ('instead', -0.0019281649032273855),\n",
       " ('getting', -0.0022166045075397807),\n",
       " ('together', -0.002353451414287863),\n",
       " ('purchase', -0.0025043340581670363),\n",
       " ('find', -0.0025268053015301568),\n",
       " ('head', -0.0025901042191508426),\n",
       " ('go', -0.002760603631110178),\n",
       " ('say', -0.0028114861958062876),\n",
       " ('fit', -0.0028196716853235537),\n",
       " ('second', -0.0028725055596068487),\n",
       " ('however', -0.0031705275712253936),\n",
       " ('come', -0.003232293307448333),\n",
       " ('problem', -0.003378769211681065),\n",
       " ('give', -0.003634640138328766),\n",
       " ('either', -0.0036949512652424852),\n",
       " ('know', -0.0037498464466025785),\n",
       " ('want', -0.003964441269037393),\n",
       " ('bottle', -0.003985804372689815),\n",
       " ('like', -0.004012674876131312),\n",
       " ('right', -0.004123767476740157),\n",
       " ('different', -0.00413158491060751),\n",
       " ('gate', -0.004160747744827084),\n",
       " ('took', -0.004298962662432533),\n",
       " ('wanted', -0.004330305782738801),\n",
       " ('weeks', -0.004350135918631167),\n",
       " ('see', -0.004352881952205973),\n",
       " ('purchased', -0.004432469763390717),\n",
       " ('away', -0.004757677185286699),\n",
       " ('last', -0.004822848415602177),\n",
       " ('came', -0.004954082869785295),\n",
       " ('buying', -0.005015087667465265),\n",
       " ('tub', -0.005185211917236045),\n",
       " ('one', -0.005240328876921257),\n",
       " ('anything', -0.005331487603879164),\n",
       " ('pump', -0.005334153984694427),\n",
       " ('less', -0.00537941890232329),\n",
       " ('make', -0.00547650019560423),\n",
       " ('actually', -0.005703037784686172),\n",
       " ('design', -0.005721227947572743),\n",
       " ('times', -0.005748162325826816),\n",
       " ('water', -0.005778113611496449),\n",
       " ('much', -0.005820865907973413),\n",
       " ('quality', -0.005857059980750963),\n",
       " ('child', -0.0059704272093479595),\n",
       " ('bottom', -0.005973778383658588),\n",
       " ('new', -0.005980909765771513),\n",
       " ('maybe', -0.00606320699663826),\n",
       " ('part', -0.006236979186493673),\n",
       " ('started', -0.006245789674640817),\n",
       " ('said', -0.006256304333645085),\n",
       " ('top', -0.006264555364781907),\n",
       " ('cup', -0.006311547231173567),\n",
       " ('picture', -0.006636610914255245),\n",
       " ('ordered', -0.006769470510477964),\n",
       " ('made', -0.006777905800962794),\n",
       " ('worked', -0.006862432126523159),\n",
       " ('went', -0.006870790546275687),\n",
       " ('working', -0.007041945901153443),\n",
       " ('open', -0.007094653976564978),\n",
       " ('box', -0.007187301221822917),\n",
       " ('milk', -0.007394638131350184),\n",
       " ('stay', -0.008696504750903321),\n",
       " ('idea', -0.008759235359703656),\n",
       " ('completely', -0.009194496191269351),\n",
       " ('unit', -0.009343712689748292),\n",
       " ('bad', -0.009519472664880905),\n",
       " ('piece', -0.009651479482034419),\n",
       " ('trying', -0.009834205560115935),\n",
       " ('difficult', -0.009864308777979669),\n",
       " ('better', -0.0101356787362247),\n",
       " ('company', -0.010135901443394092),\n",
       " ('got', -0.010256282648006224),\n",
       " ('cheap', -0.010341090995988716),\n",
       " ('two', -0.010571957519032102),\n",
       " ('thing', -0.010636947932972407),\n",
       " ('try', -0.010677528961328389),\n",
       " ('reviews', -0.010907255968065395),\n",
       " ('going', -0.01098467522304984),\n",
       " ('another', -0.011174928562632871),\n",
       " ('first', -0.011466574278205167),\n",
       " ('received', -0.011965711520728846),\n",
       " ('broke', -0.01207863540345767),\n",
       " ('bought', -0.01221822188862161),\n",
       " ('something', -0.012262558606932185),\n",
       " ('plastic', -0.012890773552133212),\n",
       " ('hard', -0.012909947911458164),\n",
       " ('never', -0.013005405035213156),\n",
       " ('item', -0.01365514290056185),\n",
       " ('returned', -0.014763305420310346),\n",
       " ('time', -0.017036585087027243),\n",
       " ('way', -0.017469425344553206),\n",
       " ('buy', -0.01898261536532469),\n",
       " ('tried', -0.019163565631510185),\n",
       " ('back', -0.019459453150996592),\n",
       " ('could', -0.01949449976413961),\n",
       " ('disappointed', -0.019732758233784013),\n",
       " ('monitor', -0.02068181121460328),\n",
       " ('thought', -0.020756425233543412),\n",
       " ('work', -0.02094666567861414),\n",
       " ('money', -0.0223526167156249),\n",
       " ('waste', -0.02315534512199584),\n",
       " ('return', -0.024570662566524983),\n",
       " ('get', -0.02932399727767066),\n",
       " ('even', -0.030214144320453017),\n",
       " ('product', -0.03617625947473382),\n",
       " ('would', -0.0538797585447843)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_coeff_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ten \"most positive\" words \n",
    "18. Compute the 10 words that have the most positive coefficient values. These words are associated with positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'easy',\n",
       " 'great',\n",
       " 'little',\n",
       " 'loves',\n",
       " 'perfect',\n",
       " 'well',\n",
       " 'fits',\n",
       " 'nice',\n",
       " 'daughter']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_positive_words = [top_tuple[0] for top_tuple in words_coeff_tuple[:10]]\n",
    "top_10_positive_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz question: Which word is not present in the top 10 \"most positive\" words? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ten \"most negative\" words\n",
    "\n",
    "19. Next, we repeat this exerciese on the 10 most negative words. That is, we compute the 10 words that have the most negative coefficient values. These words are associated with negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buy', -0.01898261536532469),\n",
       " ('tried', -0.019163565631510185),\n",
       " ('back', -0.019459453150996592),\n",
       " ('could', -0.01949449976413961),\n",
       " ('disappointed', -0.019732758233784013),\n",
       " ('monitor', -0.02068181121460328),\n",
       " ('thought', -0.020756425233543412),\n",
       " ('work', -0.02094666567861414),\n",
       " ('money', -0.0223526167156249),\n",
       " ('waste', -0.02315534512199584),\n",
       " ('return', -0.024570662566524983),\n",
       " ('get', -0.02932399727767066),\n",
       " ('even', -0.030214144320453017),\n",
       " ('product', -0.03617625947473382),\n",
       " ('would', -0.0538797585447843)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_coeff_tuple[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz question: Which word is not present in the top 10 \"most negative\" words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
